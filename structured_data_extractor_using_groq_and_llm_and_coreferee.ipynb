{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNE7uP++6CqEYMWGY1Iy50k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shawngraham/homecooked-history/blob/main/structured_data_extractor_using_groq_and_llm_and_coreferee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook does some interesting things. I have found, through experiments, that while LLMs get us _some_ of the way towards one-shot or no-shot structured data extraction from unstructured texts (eg, historical documents, articles, reports), reworking the text with coreference resolution performed first gets us _much_ closer to what we want. And using a dedicated coreference model works much better than trying to get an LLM to do it.\n",
        "\n",
        "So this notebook demonstrates a flow, and saves our work at each step for subsequent examination.\n",
        "\n",
        "1.  use coreference resolution to sort out pronouns/noun agreement etc.; return modified text\n",
        "2. use a template with an LLM to further massage that text so that any instance of eg 'Graham' gets switched to 'Shawn Graham'; return modified text\n",
        "3. pass that modified text through a prompt that defines our desired list of predicates. Since steps 1 and 2 reduce opportunities for confusion over who or what is doing the acting, the results of 3 tend to be higher quality than would otherwise be the case. Write to csv.\n",
        "4. pass the csv through a 'checker' that marks rows that are either not well formed, or use a predicate not in our desired list.\n",
        "\n",
        "The user can then manually inspect the results to find easily rows that need fixing, and can decide how to deal with them."
      ],
      "metadata": {
        "id": "ADPhGn3R9Bkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## initialize"
      ],
      "metadata": {
        "id": "aWa54RMtobWY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HA6XyFv-oHPl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llm\n",
        "!llm install llm-groq\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.prefer_gpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WE4CtqmrSEi",
        "outputId": "68205753-2303-4c5a-d452-68919a4096a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#!pip install -U spacy\n",
        "!python3 -m pip install coreferee\n",
        "!python3 -m coreferee install en\n",
        "!python -m spacy download en_core_web_trf\n",
        "!python -m spacy download en_core_web_lg\n"
      ],
      "metadata": {
        "id": "lU8WIbLTomTI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llm keys set groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kKuTJfSoV_3",
        "outputId": "22fe76d8-4fb3-4789-ebc6-6952f284ae0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter key: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set llama3.1-70b as default model\n",
        "!llm aliases set llama3 groq-llama3.1-70b\n",
        "\n",
        "#models via groq are so much faster! You might wish to experiment with other options.\n",
        "#eg\n",
        "\n",
        "#!llm install llm-gguf\n",
        "#!llm gguf download-model https://huggingface.co/lmstudio-community/SmolLM2-1.7B-Instruct-GGUF/resolve/main/SmolLM2-1.7B-Instruct-Q8_0.gguf -a smol17\n",
        "\n",
        "# now available via llm -m smol17"
      ],
      "metadata": {
        "id": "zLrN5rwYoWpV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test text\n",
        "\n",
        "We create a folder called 'source-texts' and, for testing, put the full text for an article about Giacomo Medici into it. [via Trafficking Culture](https://traffickingculture.org/encyclopedia/case-studies/giacomo-medici/). Note - while that article begins with 'Medici started dealing' I added the word 'Giacomo' so that we don't get confused in model space about other famous Medicis.\n",
        "\n",
        "You would put your own source txt files into this folder."
      ],
      "metadata": {
        "id": "ycDOlekUoe4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir source-texts\n",
        "!echo -e \"\"\"Giacomo Medici started dealing in antiquities in Rome during the 1960s (Silver 2009: 25). In July 1967, he was convicted in Italy of receiving looted artefacts, though in the same year he met and became an important supplier of antiquities to US dealer Robert Hecht (Silver 2009: 27-9). In 1968, Medici opened the gallery Antiquaria Romana in Rome and began to explore business opportunities in Switzerland (Silver 2009: 34). It is widely believed that in December 1971 he bought the illegally-excavated Euphronios (Sarpedon) krater from tombaroli before transporting it to Switzerland and selling it to Hecht (Silver 2009: 50).\\n\\n In 1978, he closed his Rome gallery, and entered into partnership with Geneva resident Christian Boursaud, who started consigning material supplied by Medici for sale at Sotheby’s London (Silver 2009: 121-2, 139; Watson and Todeschini 2007: 27). Together, they opened Hydra Gallery in Geneva in 1983 (Silver 2009: 139). It has been estimated that throughout the 1980s Medici was the source of more consignments to Sotheby’s London than any other vendor (Watson and Todeschini 2007: 27). At any one time, Boursaud might consign anything up to seventy objects, worth together as much as £500,000 (Watson 1997: 112). Material would be delivered to Sotheby’s from Geneva by courier (Watson 1997: 112). \\n\\n In October 1985, the Hydra Gallery sold fragments of the Onesimos kylix to the J. Paul Getty Museum for $100,000, providing a false provenance by way of the fictitious Zbinden collection, a provenance that was sometimes used for material offered at Sotheby’s (Silver 2009: 145; Watson and Todeschini 2007: 95). The Getty returned the kylix to Italy in 1999. \\n\\n In 1986, bad publicity surrounding the sale of looted Apulian vases at Sotheby’s London caused Medici and Boursaud to part company, and Medici bought the Geneva-based Editions Services to continue consigning material to Sotheby’s (Silver 2009: 147; Watson and Todeschini 2007: 27; Watson 1997: 117, 183-6). From 1987 until 1994, he was also consigning material to Sotheby’s through other ‘front companies’, including Mat Securitas, Arts Franc and Tecafin Fiduciaire (Watson and Todeschini 2007: 73). He developed a triangulating system of consigning through one company and purchasing the same piece through another company. There were two potentially positive outcomes of this triangulation manoeuvre: first, it artificially created demand, suggesting to potential customers that the market was stronger than it actually was; and second, it was a way of providing illegally-excavated or -exported pieces with a ‘Sotheby’s’ provenance, and, in effect, laundering them (Watson and Todeschini 2007: 135-41). \\n\\n By the late 1980s, Medici had developed commercial relations with other major antiquities dealers including Robin Symes, Frieda Tchacos, Nikolas Koutoulakis, Robert Hecht, and the brothers Ali and Hicham Aboutaam (Watson and Todeschini 2007: 73-4). He was the ultimate source of artefacts that would subsequently be sold through dealers or auction houses to private collectors, including Lawrence and Barbara Fleischman, Maurice Tempelsman, Shelby White and Leon Levy, the Hunt brothers, George Ortiz, and José Luis Várez Fisa (Watson and Todeschini 2007: 112-34; Isman 2010), and to museums including the J. Paul Getty, the Metropolitan Museum of Art, the Cleveland Museum of Art, and the Boston Museum of Fine Arts. \\n\\n In 1995, a Sotheby’s London auction catalogue advertised for sale a sarcophagus recognized by the Carabinieri to have been stolen from the church of San Saba, in Rome. Sotheby’s informed the Carabinieri that it had been consigned by Editions Services (Watson and Todeschini 2007: 19). This was around the same time that the ‘organigram’ had been discovered, revealing Medici’s central position in the organisation of the antiquities trade out of Italy (Watson and Todeschini 2007: 19), and putting the evidence together, the Carabinieri decided to act. On 13 September 1995, in concert with Swiss police, they raided Medici’s storage space in the Geneva Freeport, which comprised five rooms with a combined area of about 200 sq metres (Silver 2009: 174; Watson and Todeschini 2007: 20). One room was equipped as a laboratory for cleaning and restoring artefacts, another was fitted out as a showroom, presumably for receiving potential customers (Silver 2009: 180-1). In January 1997, Medici was arrested in Rome (Silver 2009: 175-6), and in July 1997, his Geneva storerooms, which had remained sealed since 1995, were opened again for the process of examination and inventory. \\n\\n The official report of the contents of Medici’s storerooms was submitted in July 1999. The storerooms had been found to contain 3,800 whole or fragmentary objects, more than 4,000 photographs of artefacts, and 35,000 sheets of paper containing information relating to Medici’s business practices and connections. The artefacts were mainly from Italy, but there were also hundreds from Egypt, Syria, Greece and Asia. The Swiss authorities turned over Italian material to Italy, but returned the rest to Medici (Silver 2009: 192). The photographs were mainly Polaroids, showing what appeared to be illegally-excavated artefacts, sometimes with several views of the same one, in various stages of restoration. Some artefacts were shown still covered with dirt after their excavation, some fragmentary, and others cleaned and reassembled prior to sale (Watson and Todeschini 2007: 54-68). In 2002, Carabinieri raided Medici’s home in Santa Marinella (Watson and Todeschini 2007: 200). \\n\\n Medici was charged with receiving stolen goods, illegal export of goods, and conspiracy to traffic, and his trial in Rome commenced on 4 December 2003. On 12 May 2005, he was found guilty of all charges. The judge declared that Medici had trafficked thousands of artefacts, including the sarcophagus fragment that had started the investigation, and the Euphronios (Sarpedon) krater (Silver 2009: 212). He was sentenced to ten years in prison and received a €10 million fine, with the money going to the Italian state in compensation for damage caused to cultural heritage (Silver 2009: 214). In July 2009, an appeals court in Rome dismissed the trafficking conviction against him because of the expired limitation period, but reaffirmed the convictions for receiving and conspiracy. His jail sentence was reduced to eight years, but the €10 million fine remained in place (Scherer 2009). In December 2011, a further appeal failed (Felch 2012). \\n\\n The evidence recovered during the investigation into Medici’s business was instrumental in forcing several museums and private collectors to return artefacts to Italy, and triggered further investigations and ultimately the prosecutions of Marion True and Robert Hecht.\"\"\" > testing/giacomo.txt"
      ],
      "metadata": {
        "id": "_ThdhIATohRx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## coreference resolution\n",
        "\n",
        "Coreference resolution involves figuring out which pronouns go with what nouns, and replacing them with the nouns. So, 'John Smith worked in Ottawa. Later he moved to Montreal' _should_ become 'John Smith worked in Ottawa. Later John Smith moved to Montreal'.\n",
        "\n",
        "You might want to snoop in the 'resolved' folder to see how well this has worked."
      ],
      "metadata": {
        "id": "IoN7Z1c1okpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ok, let's try on a full folder\n",
        "\n",
        "import coreferee, spacy\n",
        "import spacy_transformers\n",
        "import os\n",
        "\n",
        "# Load the Spacy language model and add the Coreferee pipeline component\n",
        "nlp = spacy.load('en_core_web_trf')\n",
        "nlp.add_pipe('coreferee')\n",
        "\n",
        "# Define the input folder containing text files and the output folder for the resolved texts\n",
        "input_folder = \"source-texts\"  # Replace with the path to your input folder\n",
        "output_folder = \"resolved\"  # Replace with the path to your output folder\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Iterate over all text files in the input directory\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Construct the full file paths\n",
        "        input_file_path = os.path.join(input_folder, filename)\n",
        "        output_file_path = os.path.join(output_folder, filename)\n",
        "\n",
        "        # Read the content of the text file\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Process the text with Spacy and Coreferee\n",
        "        coref_doc = nlp(text)\n",
        "\n",
        "        # Perform entity co-resolution\n",
        "        resolved_text = \"\"\n",
        "        for token in coref_doc:\n",
        "            repres = coref_doc._.coref_chains.resolve(token)\n",
        "            if repres:\n",
        "                resolved_text += \" \" + \" and \".join([t.text for t in repres])\n",
        "            else:\n",
        "                resolved_text += \" \" + token.text\n",
        "\n",
        "        # Write the resolved text to the output file\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(resolved_text.strip())  # Remove leading space"
      ],
      "metadata": {
        "id": "uN9gdCx2o3CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## extract!"
      ],
      "metadata": {
        "id": "-b4VPxHFo7NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!llm -m llama3 'is this thing on'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WudZWgoHsY-U",
        "outputId": "07531e5f-fab1-4cf2-a97b-5f871a0e72f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like it is. I'm here and ready to chat. How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a template for the LLM to apply so that any time our text says something like\n",
        "\n",
        "'John Smith did x. Later Smith did y'\n",
        "\n",
        "...all references to Smith become john_smith."
      ],
      "metadata": {
        "id": "q-avP_IXu2Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the template\n",
        "!llm --system \"Replace all references to individuals in this text with a consistent firstname_surname format. Rules: 1. First full name mention: Replace with 'firstname_surname' 2. Subsequent surname-only mentions: Replace with the same 'firstname_surname' 3. Preserve original text structure and context 4. Ensure replacements are uniform throughout the text Examples - 'John Smith arrived late' -> 'firstname_surname arrived late' - 'Smith apologized' -> 'firstname_surname apologized' RETURN ONLY the modified text output.\"\"\" --save namefix3"
      ],
      "metadata": {
        "id": "hAFPTLsRsp4d"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir namefixed"
      ],
      "metadata": {
        "id": "sDjV6boWycdN"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the name fix\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "for filename in os.listdir(\"resolved\"):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        input_path = os.path.join(\"resolved\", filename)\n",
        "        output_path = os.path.join(\"namefixed\", filename[:-4] + \"_namefixed.txt\")\n",
        "\n",
        "        command = f\"cat {input_path} | llm -m llama3 -t namefix3 > {output_path}\"\n",
        "        subprocess.run(command, shell=True)"
      ],
      "metadata": {
        "id": "VJGz_9ZKsuF2"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pydantic"
      ],
      "metadata": {
        "id": "TLenv74ktOGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we do the actual extraction. Note line 41 where we specify the target relations we're after."
      ],
      "metadata": {
        "id": "OjE4ob1u7XMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import llm\n",
        "import re\n",
        "\n",
        "# Ensure results directory exists\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# Get the LLM model\n",
        "model = llm.get_model(\"llama3\")\n",
        "\n",
        "# Path to the ready-to-go folder\n",
        "input_folder = \"namefixed\"\n",
        "\n",
        "# Iterate through all text files in the ready-to-go folder\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Full path to the input file\n",
        "        input_path = os.path.join(input_folder, filename)\n",
        "\n",
        "        # Read the text content\n",
        "        with open(input_path, \"r\") as file:\n",
        "            text_content = file.read()\n",
        "\n",
        "        # Split the text content into paragraphs using regular expressions\n",
        "        paragraphs = re.split(r'\\n\\s*\\n', text_content)\n",
        "\n",
        "        # Prepare output file path in results folder\n",
        "        output_filename = filename.replace(\".txt\", \"_triplets.csv\")\n",
        "        output_path = os.path.join(\"results\", output_filename)\n",
        "\n",
        "        # Open the output file to write results\n",
        "        with open(output_path, \"w\") as output_file:\n",
        "            # Write CSV header\n",
        "            output_file.write(\"subject,verb,object\\n\")\n",
        "\n",
        "            # Iterate through each paragraph\n",
        "            for paragraph_index, paragraph in enumerate(paragraphs, 1):\n",
        "                # Construct the prompt for the current paragraph\n",
        "                # THIS IS WHERE YOU ALSO INDICATE TARGET VERBS/PREDICATES\n",
        "                # MAKE SURE THESE ARE THE SAME AS IN THE VALIDATION BLOCK IN THE NEXT CODE CELL\n",
        "                prompt = paragraph + \"\\n\\n Your output will be in csv format with columns 'subject','verb','object'. Extract subject,verb,object triplets that capture the nuance of the text. Ignore scholarly citations. The target predicates are sold_to, worked_with, purchased, sold, stole, was_responsible_for. RETURN ONLY THE LIST OF TRIPLETS.\"\n",
        "\n",
        "                # Send the prompt to the LLM and get the response\n",
        "                try:\n",
        "                    response = model.prompt(prompt, temperature=0)\n",
        "\n",
        "                    # Combine response chunks\n",
        "                    full_response = ''.join(chunk for chunk in response)\n",
        "\n",
        "                    # Print the response for the current paragraph to console\n",
        "                    print(f\"Paragraph {paragraph_index} from {filename}:\")\n",
        "                    print(full_response)\n",
        "                    print(\"\\n---\\n\")\n",
        "\n",
        "                    # Write the response to the output file\n",
        "                    output_file.write(full_response)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing paragraph {paragraph_index} in {filename}: {e}\")\n",
        "\n",
        "print(\"Extraction complete. Results saved in 'results' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t-ovF8b03ri",
        "outputId": "571ab70f-ce38-44be-e942-d6a391456dbd"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 1 from giacomo_namefixed.txt:\n",
            "giacomo_medici,was_responsible_for,dealing in antiquities\n",
            "giacomo_medici,was convicted of,receiving looted artefacts\n",
            "giacomo_medici,worked_with,robert_hecht\n",
            "giacomo_medici,purchased,Euphronios (Sarpedon) krater\n",
            "giacomo_medici,sold_to,robert_hecht\n",
            "giacomo_medici,opened,Antiquaria Romana \n",
            "tombaroli,stole,Euphronios (Sarpedon) krater\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 2 from giacomo_namefixed.txt:\n",
            "giacomo_medici,worked_with,christian_boursaud\n",
            "giacomo_medici,sold_to,Sotheby's London\n",
            "giacomo_medici,was_responsible_for,consignments to Sotheby's London\n",
            "christian_boursaud,sold_to,Sotheby's London\n",
            "giacomo_medici,worked_with,christian_boursaud at Hydra Gallery \n",
            "giacomo_medici,sold,objects to Sotheby's London\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 3 from giacomo_namefixed.txt:\n",
            "\"Hydra Gallery\",\"sold\",\"fragments of the Onesimos kylix\"\n",
            "\"Hydra Gallery\",\"sold_to\",\"J. Paul Getty Museum\"\n",
            "\"J. Paul Getty Museum\",\"purchased\",\"fragments of the Onesimos kylix\"\n",
            "\"J. Paul Getty Museum\",\"returned\",\"kylix\"\n",
            "\"J. Paul Getty Museum\",\"returned\",\"kylix to Italy\"\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 4 from giacomo_namefixed.txt:\n",
            "\"giacomo_medici\",\"sold_to\",\"Sotheby's\"\n",
            "\"giacomo_medici\",\"worked_with\",\"christian_boursaud\"\n",
            "\"giacomo_medici\",\"purchased\",\"Editions Services\"\n",
            "\"giacomo_medici\",\"sold_to\",\"Sotheby's\"\n",
            "\"giacomo_medici\",\"worked_with\",\"Mat Securitas\"\n",
            "\"giacomo_medici\",\"worked_with\",\"Arts Franc\"\n",
            "\"giacomo_medici\",\"worked_with\",\"Tecafin Fiduciaire\"\n",
            "\"giacomo_medici\",\"purchased\",\"pieces\"\n",
            "\"giacomo_medici\",\"sold\",\"pieces\"\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 5 from giacomo_namefixed.txt:\n",
            "giacomo_medici,worked_with,robin_symes\n",
            "giacomo_medici,worked_with,frieda_tchacos\n",
            "giacomo_medici,worked_with,nikolas_koutoulakis\n",
            "giacomo_medici,worked_with,robert_hecht\n",
            "giacomo_medici,worked_with,ali_aboutaam\n",
            "giacomo_medici,worked_with,hicham_aboutaam\n",
            "ali_aboutaam,sold_to,lawrence_fleischman\n",
            "ali_aboutaam,sold_to,barbara_fleischman\n",
            "ali_aboutaam,sold_to,maurice_tempelsman\n",
            "ali_aboutaam,sold_to,shelby_white\n",
            "ali_aboutaam,sold_to,leon_levy\n",
            "ali_aboutaam,sold_to,george_oriz\n",
            "ali_aboutaam,sold_to,josé_luis_várez_fisa\n",
            "ali_aboutaam,sold_to,J._Paul_Getty\n",
            "ali_aboutaam,sold_to,Metropolitan_Museum_of_Art\n",
            "ali_aboutaam,sold_to,Cleveland_Museum_of_Art\n",
            "ali_aboutaam,sold_to,Boston_Museum_of_Fine_Arts\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 6 from giacomo_namefixed.txt:\n",
            "\"Carabinieri\",\"stole\",\"sarcophagus\"\n",
            "\"Sotheby’s\",\"sold\",\"sarcophagus\"\n",
            "\"Editions Services\",\"sold_to\",\"Sotheby’s\"\n",
            "\"Giacomo Medici\",\"was_responsible_for\",\"antiquities trade\"\n",
            "\"Giacomo Medici\",\"worked_with\",\"Editions Services\"\n",
            "\"Giacomo Medici\",\"purchased\",\"antiquities\"\n",
            "\"Giacomo Medici\",\"stole\",\"antiquities\"\n",
            "\"Carabinieri\",\"raided\",\"Giacomo Medici’s storage space\"\n",
            "\"Carabinieri\",\"arrested\",\"Giacomo Medici\"\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 7 from giacomo_namefixed.txt:\n",
            "\"Swiss authorities\",\"turned over\",\"Italian material\"\n",
            "\"Giacomo Medici\",\"had\",\"whole or fragmentary objects\"\n",
            "\"Giacomo Medici\",\"had\",\"photographs of artefacts\"\n",
            "\"Giacomo Medici\",\"had\",\"sheets of paper containing information\"\n",
            "\"Giacomo Medici\",\"was responsible for\",\"illegally-excavated artefacts\"\n",
            "\"Giacomo Medici\",\"sold\",\"artefacts\"\n",
            "\"Giacomo Medici\",\"worked with\",\"dealers or collectors\"\n",
            "\"Carabinieri\",\"raided\",\"Giacomo Medici's home\"\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 8 from giacomo_namefixed.txt:\n",
            "giacomo_medici,was_responsible_for,thousands of artefacts\n",
            "giacomo_medici,was_responsible_for,sarcophagus fragment\n",
            "giacomo_medici,was_responsible_for,Euphronios ( Sarpedon ) krater\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 9 from giacomo_namefixed.txt:\n",
            "giacomo_medici,sold_to,museums and private collectors\n",
            "giacomo_medici,was_responsible_for,triggering investigations and prosecutions of marion_true and robert_hecht\n",
            "\n",
            "---\n",
            "\n",
            "Extraction complete. Results saved in 'results' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you look at the results, note that sometimes names aren't rendered with an underscore. You _could_ run the file through the 'namefix' template again to try to fix this. You'd run something like this:\n",
        "\n",
        "`!cat results/giacomo_namefixed_triplets.csv | llm -m llama3 -t namefix3 > output.csv`\n",
        "\n",
        "...choosing a sensible name/place instead of `output.csv`\n",
        "\n",
        "\n",
        "Or you could just fix this manually later.\n"
      ],
      "metadata": {
        "id": "W74y4bkT7vJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## validate results\n",
        "\n",
        "This last bit runs your results through a checker to mark up rows that DO NOT have your target predicates present or DO NOT have 3 columns of data. This way, it becomes easy for you to manually inspect the results and decide how you want to handle things.\n",
        "\n",
        "If you change your list of desired predicates back where you extract things, make sure the 'valid_predicates' list below is updated accordingly.\n"
      ],
      "metadata": {
        "id": "qSrgGzFu8U0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Timport csv\n",
        "import os\n",
        "\n",
        "def error_check_predicates(input_file, output_file, valid_predicates=None):\n",
        "    \"\"\"\n",
        "    Check CSV file for valid predicates and column count.\n",
        "\n",
        "    Args:\n",
        "    input_file (str): Path to input CSV file\n",
        "    output_file (str): Path to output error-checked file\n",
        "    valid_predicates (list): List of valid predicate verbs\n",
        "    \"\"\"\n",
        "    if valid_predicates is None:\n",
        "        valid_predicates = [\n",
        "            'sold_to', 'worked_with', 'purchased',\n",
        "            'sold', 'stole', 'was_responsible_for'\n",
        "        ]\n",
        "\n",
        "    with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
        "        reader = csv.reader(infile)\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        # Write header\n",
        "        header = next(reader)\n",
        "        writer.writerow(header)\n",
        "\n",
        "        for row in reader:\n",
        "            # Mark line with ### if more than 3 columns\n",
        "            if len(row) != 3:\n",
        "                row.insert(0, '###')\n",
        "                writer.writerow(row)\n",
        "                continue\n",
        "\n",
        "            # Check predicate validity\n",
        "            verb = row[1].strip().lower()\n",
        "            if verb not in valid_predicates:\n",
        "                row.insert(0, '###')\n",
        "\n",
        "            writer.writerow(row)\n",
        "\n",
        "def process_all_files(input_dir='results', output_dir='error-checked'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith('_triplets.csv'):\n",
        "            input_path = os.path.join(input_dir, filename)\n",
        "            output_path = os.path.join(output_dir, f'checked_{filename}')\n",
        "\n",
        "            error_check_predicates(input_path, output_path)\n",
        "            print(f'Processed: {filename}')\n",
        "\n",
        "\n",
        "\n",
        "process_all_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czwfzoZJ3mhU",
        "outputId": "fa407442-9e1c-4d74-8cb5-ed0f070841aa"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed: giacomo_namefixed_triplets.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can download your csv file with entities & relations, for use in knowledge graph embedding models, network analysis, or whatever else.\n",
        "\n",
        "Open the csv file in a text editor FIRST though, and sort the lines alphabetically. The lines marked with ### will be put at the top, and you can manually work through them to decide what to do with the predicate or the extra columns of data (where extra commas have crept in)."
      ],
      "metadata": {
        "id": "4lwow8E_55kY"
      }
    }
  ]
}