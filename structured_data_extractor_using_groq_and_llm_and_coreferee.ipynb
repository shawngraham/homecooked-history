{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shawngraham/homecooked-history/blob/main/structured_data_extractor_using_groq_and_llm_and_coreferee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save a copy of this notebook first, and then fire up your copy so you can save your work or any modifications you make.**\n",
        "\n",
        "This notebook does some interesting things. I have found, through experiments, that while LLMs get us _some_ of the way towards one-shot or no-shot structured data extraction from unstructured texts (eg, historical documents, articles, reports), reworking the text with coreference resolution performed first gets us _much_ closer to what we want. And using a dedicated coreference model works much better than trying to get an LLM to do it.\n",
        "\n",
        "This notebook demonstrates a flow, and saves our work at each step for subsequent examination.\n",
        "\n",
        "1.  use coreference resolution to sort out pronouns/noun agreement etc.; return modified text\n",
        "2. use a template with an LLM to further massage that text so that any instance of eg 'Graham' gets switched to 'Shawn Graham'; return modified text\n",
        "3. pass that modified text through a prompt that defines our desired list of predicates. Since steps 1 and 2 reduce opportunities for confusion over who or what is doing the acting, the results of 3 tend to be higher quality than would otherwise be the case. Write to csv.\n",
        "4. pass the csv through a 'checker' that marks rows that are either not well formed, or use a predicate not in our desired list.\n",
        "\n",
        "The user can then manually inspect the results to find easily rows that need fixing, and can decide how to deal with them.\n",
        "\n",
        "For access to the Llama3.1-70b model, get a free developer key from [Groq](https://console.groq.com/playground). You'll paste it in below when you run the command `llm keys set groq`. (paste, hit enter; the cell will stop running without further comment once you do).\n",
        "\n",
        "Dec 3 addition\n",
        "\n",
        "~~Seeing what happens when the preprocessed text is run through nuextract~~ nothing useful\n",
        "\n",
        "Dec 4\n",
        "\n",
        "Use Gemini. Add rate limits to stay within the lines."
      ],
      "metadata": {
        "id": "ADPhGn3R9Bkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3wIn2pideEHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## initialize"
      ],
      "metadata": {
        "id": "aWa54RMtobWY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HA6XyFv-oHPl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llm\n",
        "#!llm install llm-groq\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.prefer_gpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WE4CtqmrSEi",
        "outputId": "82b77ac6-68d8-488f-bede-d8d7b1b8e9b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade pydantic"
      ],
      "metadata": {
        "id": "TLenv74ktOGz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#!pip install -U spacy\n",
        "!python3 -m pip install coreferee\n",
        "!python3 -m coreferee install en\n",
        "!python -m spacy download en_core_web_trf\n",
        "!python -m spacy download en_core_web_lg\n"
      ],
      "metadata": {
        "id": "lU8WIbLTomTI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!llm keys set groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kKuTJfSoV_3",
        "outputId": "03d0f5cf-c81e-4dbf-8250-1f00b6dfc019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter key: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set llama3.1-70b as default model\n",
        "#!llm aliases set themodel groq-llama3.1-70b\n",
        "\n",
        "#models via groq are so much faster! You might wish to experiment with other options.\n",
        "#eg\n",
        "\n",
        "#!llm install llm-gguf\n",
        "#!llm gguf download-model https://huggingface.co/lmstudio-community/SmolLM2-1.7B-Instruct-GGUF/resolve/main/SmolLM2-1.7B-Instruct-Q8_0.gguf -a smol17\n",
        "\n",
        "# now available via llm -m smol17\n",
        "# but it _will_ be much slower"
      ],
      "metadata": {
        "id": "zLrN5rwYoWpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try llm-gemini\n",
        "!llm install llm-gemini\n",
        "\n"
      ],
      "metadata": {
        "id": "-BvOxwwZxGxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llm keys set gemini"
      ],
      "metadata": {
        "id": "TLRjRTOO0FNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llm models"
      ],
      "metadata": {
        "id": "qqreTteDxwj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llm aliases set themodel gemini-1.5-pro-002"
      ],
      "metadata": {
        "id": "g-pAFiw8yF3U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test text\n",
        "\n",
        "We create a folder called 'source-texts' and, for testing, put the full text for an article about Giacomo Medici into it. [via Trafficking Culture](https://traffickingculture.org/encyclopedia/case-studies/giacomo-medici/). Note - while that article begins with 'Medici started dealing' I added the word 'Giacomo' so that we don't get confused in model space about other famous Medicis.\n",
        "\n",
        "You would put your own source txt files into this folder."
      ],
      "metadata": {
        "id": "ycDOlekUoe4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir source-texts\n",
        "!echo -e \"\"\"Giacomo Medici started dealing in antiquities in Rome during the 1960s (Silver 2009: 25). In July 1967, he was convicted in Italy of receiving looted artefacts, though in the same year he met and became an important supplier of antiquities to US dealer Robert Hecht (Silver 2009: 27-9). In 1968, Medici opened the gallery Antiquaria Romana in Rome and began to explore business opportunities in Switzerland (Silver 2009: 34). It is widely believed that in December 1971 he bought the illegally-excavated Euphronios (Sarpedon) krater from tombaroli before transporting it to Switzerland and selling it to Hecht (Silver 2009: 50).\\n\\n In 1978, he closed his Rome gallery, and entered into partnership with Geneva resident Christian Boursaud, who started consigning material supplied by Medici for sale at Sotheby’s London (Silver 2009: 121-2, 139; Watson and Todeschini 2007: 27). Together, they opened Hydra Gallery in Geneva in 1983 (Silver 2009: 139). It has been estimated that throughout the 1980s Medici was the source of more consignments to Sotheby’s London than any other vendor (Watson and Todeschini 2007: 27). At any one time, Boursaud might consign anything up to seventy objects, worth together as much as £500,000 (Watson 1997: 112). Material would be delivered to Sotheby’s from Geneva by courier (Watson 1997: 112). \\n\\n In October 1985, the Hydra Gallery sold fragments of the Onesimos kylix to the J. Paul Getty Museum for $100,000, providing a false provenance by way of the fictitious Zbinden collection, a provenance that was sometimes used for material offered at Sotheby’s (Silver 2009: 145; Watson and Todeschini 2007: 95). The Getty returned the kylix to Italy in 1999. \\n\\n In 1986, bad publicity surrounding the sale of looted Apulian vases at Sotheby’s London caused Medici and Boursaud to part company, and Medici bought the Geneva-based Editions Services to continue consigning material to Sotheby’s (Silver 2009: 147; Watson and Todeschini 2007: 27; Watson 1997: 117, 183-6). From 1987 until 1994, he was also consigning material to Sotheby’s through other ‘front companies’, including Mat Securitas, Arts Franc and Tecafin Fiduciaire (Watson and Todeschini 2007: 73). He developed a triangulating system of consigning through one company and purchasing the same piece through another company. There were two potentially positive outcomes of this triangulation manoeuvre: first, it artificially created demand, suggesting to potential customers that the market was stronger than it actually was; and second, it was a way of providing illegally-excavated or -exported pieces with a ‘Sotheby’s’ provenance, and, in effect, laundering them (Watson and Todeschini 2007: 135-41). \\n\\n By the late 1980s, Medici had developed commercial relations with other major antiquities dealers including Robin Symes, Frieda Tchacos, Nikolas Koutoulakis, Robert Hecht, and the brothers Ali and Hicham Aboutaam (Watson and Todeschini 2007: 73-4). He was the ultimate source of artefacts that would subsequently be sold through dealers or auction houses to private collectors, including Lawrence and Barbara Fleischman, Maurice Tempelsman, Shelby White and Leon Levy, the Hunt brothers, George Ortiz, and José Luis Várez Fisa (Watson and Todeschini 2007: 112-34; Isman 2010), and to museums including the J. Paul Getty, the Metropolitan Museum of Art, the Cleveland Museum of Art, and the Boston Museum of Fine Arts. \\n\\n In 1995, a Sotheby’s London auction catalogue advertised for sale a sarcophagus recognized by the Carabinieri to have been stolen from the church of San Saba, in Rome. Sotheby’s informed the Carabinieri that it had been consigned by Editions Services (Watson and Todeschini 2007: 19). This was around the same time that the ‘organigram’ had been discovered, revealing Medici’s central position in the organisation of the antiquities trade out of Italy (Watson and Todeschini 2007: 19), and putting the evidence together, the Carabinieri decided to act. On 13 September 1995, in concert with Swiss police, they raided Medici’s storage space in the Geneva Freeport, which comprised five rooms with a combined area of about 200 sq metres (Silver 2009: 174; Watson and Todeschini 2007: 20). One room was equipped as a laboratory for cleaning and restoring artefacts, another was fitted out as a showroom, presumably for receiving potential customers (Silver 2009: 180-1). In January 1997, Medici was arrested in Rome (Silver 2009: 175-6), and in July 1997, his Geneva storerooms, which had remained sealed since 1995, were opened again for the process of examination and inventory. \\n\\n The official report of the contents of Medici’s storerooms was submitted in July 1999. The storerooms had been found to contain 3,800 whole or fragmentary objects, more than 4,000 photographs of artefacts, and 35,000 sheets of paper containing information relating to Medici’s business practices and connections. The artefacts were mainly from Italy, but there were also hundreds from Egypt, Syria, Greece and Asia. The Swiss authorities turned over Italian material to Italy, but returned the rest to Medici (Silver 2009: 192). The photographs were mainly Polaroids, showing what appeared to be illegally-excavated artefacts, sometimes with several views of the same one, in various stages of restoration. Some artefacts were shown still covered with dirt after their excavation, some fragmentary, and others cleaned and reassembled prior to sale (Watson and Todeschini 2007: 54-68). In 2002, Carabinieri raided Medici’s home in Santa Marinella (Watson and Todeschini 2007: 200). \\n\\n Medici was charged with receiving stolen goods, illegal export of goods, and conspiracy to traffic, and his trial in Rome commenced on 4 December 2003. On 12 May 2005, he was found guilty of all charges. The judge declared that Medici had trafficked thousands of artefacts, including the sarcophagus fragment that had started the investigation, and the Euphronios (Sarpedon) krater (Silver 2009: 212). He was sentenced to ten years in prison and received a €10 million fine, with the money going to the Italian state in compensation for damage caused to cultural heritage (Silver 2009: 214). In July 2009, an appeals court in Rome dismissed the trafficking conviction against him because of the expired limitation period, but reaffirmed the convictions for receiving and conspiracy. His jail sentence was reduced to eight years, but the €10 million fine remained in place (Scherer 2009). In December 2011, a further appeal failed (Felch 2012). \\n\\n The evidence recovered during the investigation into Medici’s business was instrumental in forcing several museums and private collectors to return artefacts to Italy, and triggered further investigations and ultimately the prosecutions of Marion True and Robert Hecht.\"\"\" > source-texts/giacomo.txt\n",
        "!echo -e \"\"\"Marion True was appointed curatorial assistant at the J. Paul Getty Museum in 1982, under the supervision of Jiri Frel, the then Curator of Antiquities. After Frel’s departure in 1986, True was promoted to replace him as curator (Felch and Frammolino 2011: 77-8). During the period of her curatorship, True was responsible for some controversial acquisitions of unprovenanced material, including the 1988 purchase of the Getty Aphrodite, the 1993 purchase of a fourth-century BC gold funerary wreath from Greece, and the 1996 acquisition of the Barbara and Lawrence Fleischman Collection. She also rejected some potentially high-profile assemblages, including the Kanakaria mosaics in 1988, which she recognized as stolen (Felch and Frammolino 2011: 115-17), and the Sevso Treasure, when she discovered that the accompanying documents of provenance were probably forgeries (True 1997: 140). She was prepared to return material to its country of origin if it was shown convincingly to have an illicit provenance, including several hundred ceramic fragments acquired by donation between 1979 and 1981, that in 1994 were found to have been looted from a sanctuary at Francavilla Maritima in Italy (Lyons 2010: 422-5; True 1997: 143). True was the driving force behind the Getty’s adoption of clear policy guidelines as regards its acquisition of unprovenanced antiquities. The first version, in 1987, which was believed at the time by True to be the only policy of its kind in place at a major collecting museum, required the Getty to notify in writing the appropriate authority of a possible country of origin about a potential acquisition, and request information about theft or illegal export. The acquisition would only proceed if no such information was forthcoming. Furthermore, if at any time after acquisition a country could make a verifiable claim of theft or illegal export, the Getty would return the object in question, notwithstanding any legal protection offered by a statute of limitations (True 1997: 138). This policy was strengthened in November 1995 by the assurance that an unprovenanced object would only be acquired if it had been published or otherwise publicly documented as out of its country of origin prior to November 1995 (True 1997: 138). This requirement for public documentation was intended to protect against forged provenances, which True believed were rife in the antiquities trade (Kaufman 1996). Several pieces were returned in accordance with the policy (Lee 1999). Nevertheless, the sincerity of the Getty’s motives was called into question by the 1996 acquisition of the Fleischman Collection, comprising largely unprovenanced material, which had been published only in 1994 by the Getty itself. True was offered the position of Curator of Greek and Roman Art at the Metropolitan Museum in New York in 1991 after Dietrich von Bothmer’s retirement the year before, but declined the offer when she was presented with the opportunity to oversee the projected renovation and redesign of the Getty Villa in Malibu, which was to house the Getty’s antiquities collection (Eakin 2007). The project cost $275 million and the Villa reopened in January 2006 (Felch and Frammolino 2011: 273-7). True suggested that the 1995 revision of the acquisitions policy was part of a larger change in mission occasioned by this reimagining of the Getty Villa, which envisaged a shift in primary purpose for the museum from collecting antiquities to conservation abroad and incoming loan exhibitions (Somers Cocks 1995: 6). In 1995, True bought a house on the Greek island of Paros with the help of a four-year loan of $400,000 from Christos Michaelides, partner of antiquities dealer Robin Symes, from whom the Getty had bought several important pieces, including the Aphrodite (Felch and Frammolino 2011: 135-8; Watson and Todeschini 2007: 288-9). Just days after the acquisition of the Fleischman Collection in 1996, she paid back Michaelides with money borrowed from the Fleischmans on a twenty-year mortgage (Felch and Frammolino 2011: 146). When these loans came to the attention of the Getty trustees in September 2005, True was fired for failing to declare them in contravention of the Getty’s conflict-of-interest policy (Felch and Frammolino 2011: 266). On 1 April 2005, a few months before her dismissal, True was charged in Italy with receiving stolen antiquities and conspiring with dealers Robert Hecht and Giacomo Medici to receive stolen antiquities, and she was ordered to stand trial in Rome (Felch and Frammolino 2011: 259; Wilkinson and Muchnic 2005). The case against True had materialized as Italian investigators working through photographic and documentary material seized from Medici’s Geneva storerooms began to suspect her involvement. It was quickly recognized that True had acquired a  fifth-century BC bronze tripod and candelabrum for the Getty in 1990 that had been stolen from the long established Guglielmo Collection in Italy (Felch and Frammolino 2011: 153-4). Letters were also discovered revealing what appeared to be friendly relations between True, Medici and Hecht (Felch and Frammolino 2011: 212-13; Watson and Todeschini 2007: 85, 98). Finally, there was a set of photographs recording forty-two objects that had passed through the hands of Medici before ultimately being acquired by the Getty (Felch and Frammolino 2011: 197; Watson and Todeschini 2007: 87). The investigators also came to believe that prior to 1996 True had been encouraging the Fleischmans to buy objects of dubious provenance in the knowledge that they would ultimately be donated to the Getty—in effect, using the Fleischman Collection to launder potentially illicit material (Felch and Frammolino 2011: 257-9). True argued in her defence that staying on good terms with antiquities dealers was a professional requirement of her position as curator (True 2011), that she had not acquired objects for her own benefit, but for the museum, and that she should not take sole responsibility for acquisitions made during her tenure as curator, as they had all been made with the approval of the Getty CEO (Harold Williams until the end of 1997, Barry Munitz until 2006), Director (John Walsh until September 2000, Deborah Gribbon until 2004), in-house counsel, and Board of Trustees (Eakin 2010). This statement was supported by internal Getty documentation (Felch and Frammolino 2011: 218). Indeed, many of the objects in the photographs seized at Medici’s storerooms had been acquired before True was curator (Felch and Frammolino 2011: 198, 248). Both True and Barbara Fleischman rejected out of hand any imputation of collusion (Felch and Frammolino 2011: 254-5). The trial commenced on 16 November 2005, and was abandoned without verdict on 13 October 2010 as the limitation period on True’s alleged offences expired (Eakin 2010; Felch and Frammolino 2011: 312). True complained that she had been ‘neither condemned nor vindicated’ (True 2011). In November 2006, Greek prosecutors charged True in connection with the fourth-century BC gold funerary wreath acquired in 1993, which was by then believed to have been taken out of Greece illegally (Felch and Frammolino 2011: 290; Zirganos 2007: 320). In November 2007, her trial was ended without resolution after the expiry of the statute of limitations (Felch 2007; Felch and Frammolino 2011: 306) .\"\"\"  > source-texts/true.txt"
      ],
      "metadata": {
        "id": "_ThdhIATohRx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pre-process the text\n",
        "\n",
        "We're going to do a bit of munging to make sure the text is as good as it's going to get before we try extracting data"
      ],
      "metadata": {
        "id": "-b4VPxHFo7NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!llm -m themodel 'is this thing on? Which model are you?'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WudZWgoHsY-U",
        "outputId": "d87bb3ce-ffbb-4e9f-b161-71b6d5c85517"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, this thing is on!  I'm a large language model, trained by Google.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a template called 'namefix' for the LLM to apply so that any time our text says something like\n",
        "\n",
        "'John Smith did x. Later Smith did y'\n",
        "\n",
        "...all references to 'John Smith' _or_ 'Smith' become john_smith. Ditto for organizations."
      ],
      "metadata": {
        "id": "q-avP_IXu2Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the template\n",
        "!llm --system \"\"\"Replace all references to individuals in this text with a consistent firstname_surname format. Rules: 1. First full name mention: Replace with 'firstname_surname' 2. Subsequent surname-only mentions: Replace with the same 'firstname_surname' 3. Preserve original text structure and context 4. Ensure replacements are uniform throughout the text Examples - 'Trudy True arrived late' -> 'Trudy_True arrived late' - 'True apologized' -> 'Trudy_True apologized' RETURN ONLY the modified text output.\"\"\" --save namefix4\n"
      ],
      "metadata": {
        "id": "hAFPTLsRsp4d"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llm --system \"\"\"Replace all references to organizations in this text with a consistent full_name format. Rules: 1. Do not adjust personal individual names. 2. First full organization name -> mention: Replace with 'firstword_secondword' and so on for the full organization name 3. Subsequent shortname-only mentions: Replace with 'firstword_secondword' and so on for the full organization name 4. Expand abbreviations fully when they refer to such an organization. 5. Preserve original text structure and context 6. Ensure replacements are uniform throughout the text. Examples -'The Ottawa Art Gallery opened in 2010' -> 'The Ottawa_Art_Gallery opened in 2010'. RETURN ONLY the modified text output.\"\"\"  --save orgnamefix4"
      ],
      "metadata": {
        "id": "9AaODZyl0Xxc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a template to remove scholarly citations, which can cause trouble elsewhere."
      ],
      "metadata": {
        "id": "hqUUAZVtxgCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##a template for removing scholarly citations\n",
        "!llm --system \"Return the complete text but remove scholarly citations. Scholarly citations look similar to this: (Jung, 2010, p. 4) and are often a surname, a date, a page range in parenthesis. Rules: 1. Do not adjust personal individual names. 2. When encountering a scholarly citation, remove it. Example - 'John_Smith argued differently (Jones 2012).' -> 'John_Smith argued differently.'\" --save cleaner"
      ],
      "metadata": {
        "id": "t3M1ij3PNWOv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llm templates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpJNXZ8bzBy1",
        "outputId": "c785e1c4-3510-40c6-d985-b2e4938f856d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cleaner     : system: Return the complete text but remove scholarly citations. Scholarly citation...\n",
            "namefix4    : system: Replace all references to individuals in this text with a consistent firstn...\n",
            "orgnamefix4 : system: Replace all references to organizations in this text with a consistent full...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#source-texts -> namefixed -> orgfixed -> cleaned -> resolved -> results -> checked -> manually fix things -> finished\n",
        "!mkdir namefixed\n",
        "!mkdir orgfixed\n",
        "!mkdir cleaned"
      ],
      "metadata": {
        "id": "sDjV6boWycdN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the name fix\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "for filename in os.listdir(\"source-texts\"):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        input_path = os.path.join(\"source-texts\", filename)\n",
        "        output_path = os.path.join(\"namefixed\", filename[:-4] + \"_namefixed.txt\")\n",
        "\n",
        "        command = f\"cat {input_path} | llm -m themodel -t namefix4 > {output_path}\"\n",
        "        subprocess.run(command, shell=True)\n",
        "\n",
        "for filename in os.listdir(\"namefixed\"):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        input_path = os.path.join(\"namefixed\", filename)\n",
        "        output_path = os.path.join(\"orgfixed\", filename[:-4] + \"_orgfixed.txt\")\n",
        "\n",
        "        command = f\"cat {input_path} | llm -m themodel -t orgnamefix4 > {output_path}\"\n",
        "        subprocess.run(command, shell=True)\n",
        "\n",
        "\n",
        "for filename in os.listdir(\"orgfixed\"):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        input_path = os.path.join(\"orgfixed\", filename)\n",
        "        output_path = os.path.join(\"cleaned\", filename[:-4] + \"_cleaned.txt\")\n",
        "\n",
        "        command = f\"cat {input_path} | llm -m themodel -t cleaner > {output_path}\"\n",
        "        subprocess.run(command, shell=True)"
      ],
      "metadata": {
        "id": "VJGz_9ZKsuF2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## coreference resolution\n",
        "\n",
        "Coreference resolution involves figuring out which pronouns go with what nouns, and replacing them with the nouns. So, 'John Smith worked in Ottawa. Later he moved to Montreal' _should_ become 'John Smith worked in Ottawa. Later John Smith moved to Montreal'.\n",
        "\n",
        "You might want to snoop in the 'resolved' folder to see how well this has worked."
      ],
      "metadata": {
        "id": "IoN7Z1c1okpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ok, let's try on a full folder\n",
        "\n",
        "import coreferee, spacy\n",
        "import spacy_transformers\n",
        "import os\n",
        "\n",
        "# Load the Spacy language model and add the Coreferee pipeline component\n",
        "nlp = spacy.load('en_core_web_trf')\n",
        "nlp.add_pipe('coreferee')\n",
        "\n",
        "# Define the input folder containing text files and the output folder for the resolved texts\n",
        "input_folder = \"cleaned\"  # Replace with the path to your input folder\n",
        "output_folder = \"resolved\"  # Replace with the path to your output folder\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Iterate over all text files in the input directory\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Construct the full file paths\n",
        "        input_file_path = os.path.join(input_folder, filename)\n",
        "        output_file_path = os.path.join(output_folder, filename)\n",
        "\n",
        "        # Read the content of the text file\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Process the text with Spacy and Coreferee\n",
        "        coref_doc = nlp(text)\n",
        "\n",
        "        # Perform entity co-resolution\n",
        "        resolved_text = \"\"\n",
        "        for token in coref_doc:\n",
        "            repres = coref_doc._.coref_chains.resolve(token)\n",
        "            if repres:\n",
        "                resolved_text += \" \" + \" and \".join([t.text for t in repres])\n",
        "            else:\n",
        "                resolved_text += \" \" + token.text\n",
        "\n",
        "        # Write the resolved text to the output file\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(resolved_text.strip())  # Remove leading space"
      ],
      "metadata": {
        "id": "uN9gdCx2o3CF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "759062fb-f6dc-4c91-8f0c-ea6ac3df73b2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_trf' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  \"\"\"Check if string maps to a package installed via pip.\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  elif val == 1:\n",
            "/usr/local/lib/python3.10/dist-packages/spacy_transformers/layers/hf_shim.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_lg' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  \"\"\"Check if string maps to a package installed via pip.\n",
            "/usr/local/lib/python3.10/dist-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "Dec 4 - let's see if we can work out from the text a set of predicates that capture the antiquities trade\n"
      ],
      "metadata": {
        "id": "Y6Xpi20winHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import llm\n",
        "\n",
        "model = llm.get_model(\"themodel\")\n",
        "\n",
        "def suggest_predicates_rules(text):\n",
        "    \"\"\"Suggests predicates based on keywords and patterns in the text.\"\"\"\n",
        "    suggested_predicates = set()\n",
        "\n",
        "    # Rule 1: Look for common verbs associated with transactions\n",
        "    transaction_verbs = r\"(?:bought|sold|traded|acquired|consigned|purchased|exported|imported|donated|received|transferred)\"\n",
        "    matches = re.findall(transaction_verbs, text, re.IGNORECASE)\n",
        "    suggested_predicates.update(matches)\n",
        "\n",
        "    # Rule 2: Look for verbs indicating collaboration or connection\n",
        "    collaboration_verbs = r\"(?:worked with|partnered with|collaborated with|associated with|connected to|supplied to|met with)\"\n",
        "    matches = re.findall(collaboration_verbs, text, re.IGNORECASE)\n",
        "    suggested_predicates.update(matches)\n",
        "\n",
        "    # Rule 3: Look for verbs related to legal actions\n",
        "    legal_verbs = r\"(?:charged with|convicted of|sentenced to|arrested for|investigated for)\"\n",
        "    matches = re.findall(legal_verbs, text, re.IGNORECASE)\n",
        "    suggested_predicates.update(matches)\n",
        "\n",
        "\n",
        "    # Rule 4: Look for location words implying operation\n",
        "    location_verbs = r\"(?:operated in|located in|based in|established in)\"\n",
        "    matches = re.findall(location_verbs, text, re.IGNORECASE)\n",
        "    suggested_predicates.update(matches)\n",
        "\n",
        "\n",
        "    # Rule 5: Look for ownership and provenance\n",
        "    ownership_verbs = r\"(?:owned by|belonged to|originated from|stolen from|traced to)\"\n",
        "    matches = re.findall(ownership_verbs, text, re.IGNORECASE)\n",
        "    suggested_predicates.update(matches)\n",
        "\n",
        "    return list(suggested_predicates)\n",
        "\n",
        "import llm\n",
        "\n",
        "def refine_predicates_llm(suggested_predicates, text):\n",
        "    \"\"\"Refines the suggested predicates using an LLM.\"\"\"\n",
        "    prompt = f\"\"\"The following predicates were suggested for extracting relationships from a text about the antiquities trade: {', '.join(suggested_predicates)}.  The text is:  {text}.  Refine this list, removing irrelevant predicates, adding any crucial missing predicates relevant to the antiquities trade (focus on key players, institutions, objects, and transactions), and prioritizing predicates that will illuminate the key aspects of the trade network. Return the refined list as a comma-separated string. Provide a brief rationale.\"\"\"\n",
        "    refined_predicates = model.prompt(prompt, temperature=0)\n",
        "    return refined_predicates\n",
        "\n",
        "def select_predicates(text):\n",
        "    \"\"\"Selects predicates using a hybrid rule-based and LLM approach.\"\"\"\n",
        "    initial_suggestions = suggest_predicates_rules(text)\n",
        "    refined_predicates = refine_predicates_llm(initial_suggestions, text)\n",
        "    return refined_predicates\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A7RVXoYLit9U"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "text = open(\"/content/resolved/giacomo_namefixed_orgfixed_cleaned.txt\", \"r\").read()\n",
        "predicates = select_predicates(text)\n",
        "print(f\"Selected Predicates: {predicates}\")\n",
        "\n",
        "# Now use these predicates in the triple extraction step."
      ],
      "metadata": {
        "id": "o1YYXXfZi2cC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada9ffe7-fcd9-41db-d436-d0ec68dd36d1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Predicates: **Refined Predicate List:**\n",
            "\n",
            "sold, bought, consigned, received, exported, imported, looted, excavated, restored, supplied, owned, possessed,  trafficked, stolen_from,  returned,  charged_with, sentenced_to, located_at, associated_with\n",
            "\n",
            "**Rationale:**\n",
            "\n",
            "* **Removed:** \"dealing in\" is too general.  The other predicates capture the specific activities within \"dealing.\"\n",
            "* **Added:**\n",
            "    * **imported:**  Essential counterpart to exported, especially given the international nature of the trade.\n",
            "    * **looted, excavated:**  These specify the illegal origins of many items.\n",
            "    * **restored:**  Key activity in preparing objects for sale and disguising their origins.\n",
            "    * **supplied:** Captures the relationship between Medici and other dealers/auction houses.\n",
            "    * **owned, possessed:**  Important for establishing provenance and responsibility.\n",
            "    * **trafficked:**  A more serious charge than simply buying/selling.\n",
            "    * **returned:**  Crucial for tracking the repatriation of looted items.\n",
            "    * **located_at:**  Helps to map the movement of objects and establish connections between individuals and institutions.\n",
            "    * **associated_with:** A broader predicate to capture less direct relationships.\n",
            "\n",
            "* **Prioritized:** The order reflects the flow of antiquities from excavation to market, emphasizing the illegal activities and the eventual consequences.  Predicates like \"charged_with\" and \"sentenced_to\" are important but come later in the chain of events.  \"Associated_with\" is last as it's the most general and can be used to fill in gaps or explore peripheral connections.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extractor!"
      ],
      "metadata": {
        "id": "SbTxA__ux-6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we do the actual extraction. Note the array below where we specify the target relations we're after."
      ],
      "metadata": {
        "id": "OjE4ob1u7XMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_list_predicates = [\n",
        "    \"dealerIn\",\n",
        "    \"convictedOf\",\n",
        "    \"operatedBusiness\",\n",
        "    \"soldTo\",\n",
        "    \"sourceOf\",\n",
        "    \"chargedWith\",\n",
        "    \"sentencedTo\",\n",
        "    \"tradedThrough\",\n",
        "    \"locatedIn\",\n",
        "    \"suppliedArtifactsTo\",\n",
        "    \"businessPartnerOf\",\n",
        "    \"tradeConnectionWith\",\n",
        "    \"soldViaIntermediary\",\n",
        "    \"consignedThrough\",\n",
        "    \"providedArtifactsTo\",\n",
        "    \"collaboratedWith\",\n",
        "    \"introducedTo\"\n",
        "    ]\n",
        "predicates = [\"involvedIn\", \"transactedWith\", \"connectedTo\", \"legalStatus\", \"originatedFrom\", \"operatedIn\"]\n",
        "# maybe should use the old schema\n",
        "\n",
        "suggested_predicates =['sold_object_to', 'bought_object', 'purchased_from', 'consigned', 'received', 'exported', 'imported', 'looted', 'excavated', 'restored', 'supplied', 'owned', 'possessed',  'trafficked', 'stolen_from',  'returned',  'charged_with', 'sentenced_to', 'located_at', 'associated_with']"
      ],
      "metadata": {
        "id": "P-PQmlBt5HBh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "change line 33 below to whatever predicates you want"
      ],
      "metadata": {
        "id": "8B-s-hPhCcL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import llm\n",
        "import re\n",
        "\n",
        "# Ensure results directory exists\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# Get the LLM model\n",
        "model = llm.get_model(\"themodel\")\n",
        "\n",
        "# Path to the ready-to-go folder\n",
        "input_folder = \"resolved\"\n",
        "\n",
        "# Iterate through all text files in the ready-to-go folder\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Full path to the input file\n",
        "        input_path = os.path.join(input_folder, filename)\n",
        "\n",
        "        # Read the text content\n",
        "        with open(input_path, \"r\") as file:\n",
        "            text_content = file.read()\n",
        "\n",
        "        # Split the text content into paragraphs using regular expressions\n",
        "        # This is so that everything fits inside the context window\n",
        "        paragraphs = re.split(r'\\n\\s*\\n', text_content)\n",
        "\n",
        "        # Prepare output file path in results folder\n",
        "        output_filename = filename.replace(\".txt\", \"_triplets.csv\")\n",
        "        output_path = os.path.join(\"results\", output_filename)\n",
        "\n",
        "        # Convert the predicates list to a comma-separated string\n",
        "        predicates_str = \", \".join(suggested_predicates)  ### change predicates here\n",
        "\n",
        "        # Open the output file to write results\n",
        "        with open(output_path, \"w\") as output_file:\n",
        "            # Write CSV header\n",
        "            output_file.write(\"subject,verb,object\\n\")\n",
        "\n",
        "            # Iterate through each paragraph\n",
        "            for paragraph_index, paragraph in enumerate(paragraphs, 1):\n",
        "                # Construct the prompt for the current paragraph\n",
        "                # THIS IS WHERE YOU ALSO INDICATE TARGET VERBS/PREDICATES\n",
        "                # MAKE SURE THESE ARE THE SAME AS IN THE VALIDATION BLOCK IN THE NEXT CODE CELL\n",
        "               #prompt = paragraph + f\"\\n\\n Your output will be in csv format with columns 'subject','verb','object'. Extract subject,verb,object triplets that capture the nuance of the text. IGNORE SCHOLARLY PARENTHETICAL CITATIONS. The target predicates are {predicates_str}. RETURN ONLY THE LIST OF TRIPLETS.\"\n",
        "                prompt = paragraph + f\"\\n\\n Extract subject,verb,object triplets that capture the nuance of the text. Your output will be in csv format with columns 'subject','verb','object'. STRICT RULES FOR ENTITY EXTRACTION: - Use only substantive, named entities from the main text The target predicates are {predicates_str}. RETURN ONLY THE LIST OF TRIPLETS. Here is the text to process\"\n",
        "\n",
        "\n",
        "                # Send the prompt to the LLM and get the response\n",
        "                try:\n",
        "                    response = model.prompt(prompt, temperature=0)\n",
        "\n",
        "                    # Combine response chunks\n",
        "                    full_response = ''.join(chunk for chunk in response)\n",
        "\n",
        "                    # Print the response for the current paragraph to console\n",
        "                    print(f\"Paragraph {paragraph_index} from {filename}:\")\n",
        "                    print(full_response)\n",
        "                    print(\"\\n---\\n\")\n",
        "\n",
        "                    # Write the response to the output file\n",
        "                    output_file.write(full_response)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing paragraph {paragraph_index} in {filename}: {e}\")\n",
        "\n",
        "print(\"Extraction complete. Results saved in 'results' folder.\")"
      ],
      "metadata": {
        "id": "-t-ovF8b03ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c63693-6bfa-459f-c99c-67b245812ddb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph 1 from true_namefixed_orgfixed_cleaned.txt:\n",
            "```csv\n",
            "subject,verb,object\n",
            "Marion_True,appointed,curatorial assistant\n",
            "Marion_True,supervised by,Jiri_Frel\n",
            "Marion_True,promoted to,curator\n",
            "J_Paul_Getty_Museum,purchased,Getty Aphrodite\n",
            "J_Paul_Getty_Museum,purchased,gold funerary wreath\n",
            "J_Paul_Getty_Museum,acquired,Barbara_and_Lawrence_Fleischman_Collection\n",
            "Marion_True,rejected,Kanakaria mosaics\n",
            "Marion_True,rejected,Sevso Treasure\n",
            "Marion_True,returned,ceramic fragments\n",
            "J_Paul_Getty_Museum,adopted,policy guidelines\n",
            "J_Paul_Getty_Museum,required,notification\n",
            "J_Paul_Getty_Museum,would return,object\n",
            "J_Paul_Getty_Museum,strengthened,policy\n",
            "Marion_True,believed,forged provenances\n",
            "J_Paul_Getty_Museum,returned,Several pieces\n",
            "J_Paul_Getty_Museum,acquired,Barbara_and_Lawrence_Fleischman_Collection\n",
            "J_Paul_Getty_Museum,published,Barbara_and_Lawrence_Fleischman_Collection\n",
            "Marion_True,offered,position of Curator\n",
            "Marion_True,declined,offer\n",
            "Dietrich_von_Bothmer,presented with,opportunity\n",
            "Marion_True,suggested,revision\n",
            "Getty Villa,envisaged,shift\n",
            "Marion_True,bought,house\n",
            "Marion_True,borrowed from,Christos_Michaelides\n",
            "J_Paul_Getty_Museum,bought from,Robin_Symes\n",
            "Marion_True,paid back,Christos_Michaelides\n",
            "Marion_True,borrowed from,Barbara_and_Lawrence_Fleischmans\n",
            "J_Paul_Getty_Museum,fired,Marion_True\n",
            "Italy,charged,Marion_True\n",
            "Marion_True,ordered to stand trial in,Rome\n",
            "Giacomo_Medici,seized,photographic and documentary material\n",
            "Marion_True,acquired,bronze tripod and candelabrum\n",
            "Guglielmo_Collection,stolen from,bronze tripod and candelabrum\n",
            "Marion_True,associated_with,Giacomo_Medici\n",
            "Marion_True,associated_with,Robert_Hecht\n",
            "Giacomo_Medici,handled,forty-two objects\n",
            "J_Paul_Getty_Museum,acquired,forty-two objects\n",
            "Marion_True,encouraged,Barbara_and_Lawrence_Fleischmans\n",
            "Marion_True,acquired,objects\n",
            "J_Paul_Getty_Museum,made,acquisitions\n",
            "Harold_Williams,approved,acquisitions\n",
            "Barry_Munitz,approved,acquisitions\n",
            "John_Walsh,approved,acquisitions\n",
            "Deborah_Gribbon,approved,acquisitions\n",
            "Board_of_Trustees,approved,acquisitions\n",
            "Giacomo_Medici,seized,photographs\n",
            "J_Paul_Getty_Museum,acquired,objects\n",
            "Marion_True,rejected,imputation of collusion\n",
            "Barbara_Fleischman,rejected,imputation of collusion\n",
            "Greece,charged,Marion_True\n",
            "```\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 1 from giacomo_namefixed_orgfixed_cleaned.txt:\n",
            "subject,verb,object\n",
            "Giacomo_Medici,received,looted artefacts\n",
            "Giacomo_Medici,supplied,antiquities\n",
            "Giacomo_Medici,sold,krater\n",
            "Giacomo_Medici,bought,Euphronios (Sarpedon) krater\n",
            "Giacomo_Medici,supplied,antiquities\n",
            "tombaroli,excavated,Euphronios (Sarpedon) krater\n",
            "Giacomo_Medici,sold,krater\n",
            "Robert_Hecht,bought,krater\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 2 from giacomo_namefixed_orgfixed_cleaned.txt:\n",
            "```csv\n",
            "subject,verb,object\n",
            "Giacomo_Medici,closed,Rome gallery\n",
            "Giacomo_Medici,entered into partnership with,Christian_Boursaud\n",
            "Christian_Boursaud,consigned,material\n",
            "Christian_Boursaud,started,Hydra_Gallery\n",
            "Giacomo_Medici,supplied,material\n",
            "Sotheby’s_London,received,material\n",
            "Christian_Boursaud,consigned,objects\n",
            "```\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Error processing paragraph 3 in giacomo_namefixed_orgfixed_cleaned.txt: Resource has been exhausted (e.g. check quota).\n",
            "Paragraph 4 from giacomo_namefixed_orgfixed_cleaned.txt:\n",
            "```csv\n",
            "subject,verb,object\n",
            "Giacomo_Medici,bought,Editions_Services\n",
            "Giacomo_Medici,consigned,material\n",
            "Giacomo_Medici,consigned,material\n",
            "Giacomo_Medici,developed,system\n",
            "Sotheby’s_London,sold,vases\n",
            "```\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 5 from giacomo_namefixed_orgfixed_cleaned.txt:\n",
            "```csv\n",
            "subject,verb,object\n",
            "Giacomo_Medici,associated_with,Robin_Symes\n",
            "Giacomo_Medici,associated_with,Frieda_Tchacos\n",
            "Giacomo_Medici,associated_with,Nikolas_Koutoulakis\n",
            "Giacomo_Medici,associated_with,Robert_Hecht\n",
            "Giacomo_Medici,associated_with,Ali_Aboutaam\n",
            "Giacomo_Medici,associated_with,Hicham_Aboutaam\n",
            "Giacomo_Medici,supplied,Lawrence_Fleischman\n",
            "Giacomo_Medici,supplied,Barbara_Fleischman\n",
            "Giacomo_Medici,supplied,Maurice_Tempelsman\n",
            "Giacomo_Medici,supplied,Shelby_White\n",
            "Giacomo_Medici,supplied,Leon_Levy\n",
            "Giacomo_Medici,supplied,Hunt_brothers\n",
            "Giacomo_Medici,supplied,George_Ortiz\n",
            "Giacomo_Medici,supplied,José_Luis_Várez_Fisa\n",
            "Giacomo_Medici,supplied,J._Paul_Getty_Museum\n",
            "Giacomo_Medici,supplied,Metropolitan_Museum_of_Art\n",
            "Giacomo_Medici,supplied,Cleveland_Museum_of_Art\n",
            "Giacomo_Medici,supplied,Boston_Museum_of_Fine_Arts\n",
            "```\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 6 from giacomo_namefixed_orgfixed_cleaned.txt:\n",
            "```csv\n",
            "subject,verb,object\n",
            "Sotheby’s_London,sold_object_to,sarcophagus\n",
            "Carabinieri,recognized,sarcophagus\n",
            "sarcophagus,stolen_from,church of San_Saba\n",
            "Sotheby’s_London,consigned,Editions_Services\n",
            "Carabinieri,raided,storage space\n",
            "Giacomo_Medici,owned,storage space\n",
            "storage space,located_at,Geneva_Freeport\n",
            "Giacomo_Medici,arrested_in,Rome\n",
            "Giacomo_Medici,owned,storerooms\n",
            "storerooms,located_at,Geneva_Freeport\n",
            "```\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 7 from giacomo_namefixed_orgfixed_cleaned.txt:\n",
            "```csv\n",
            "subject,verb,object\n",
            "Swiss authorities,returned,rest\n",
            "Swiss authorities,turned over,Italian material\n",
            "Carabinieri,raided,Giacomo_Medici’s home\n",
            "Giacomo_Medici’s home,located_at,Santa_Marinella\n",
            "Giacomo_Medici,owned,storerooms\n",
            "storerooms,located_at,Geneva_Freeport\n",
            "storerooms,contained,objects\n",
            "storerooms,contained,photographs\n",
            "storerooms,contained,sheets\n",
            "photographs,showed,artefacts\n",
            "artefacts,located_at,Italy\n",
            "artefacts,located_at,Egypt\n",
            "artefacts,located_at,Syria\n",
            "artefacts,located_at,Greece\n",
            "artefacts,located_at,Asia\n",
            "```\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 8 from giacomo_namefixed_orgfixed_cleaned.txt:\n",
            "```csv\n",
            "subject,verb,object\n",
            "Giacomo_Medici,charged_with,receiving stolen goods\n",
            "Giacomo_Medici,charged_with,illegal export of goods\n",
            "Giacomo_Medici,charged_with,conspiracy to traffic\n",
            "Giacomo_Medici,trafficked,artefacts\n",
            "Giacomo_Medici,trafficked,sarcophagus fragment\n",
            "Giacomo_Medici,trafficked,Euphronios (Sarpedon) krater\n",
            "Giacomo_Medici,sentenced_to,ten years in prison\n",
            "Giacomo_Medici,received,€ 10 million fine\n",
            "money,going to,Italian state\n",
            "Giacomo_Medici,convicted of,receiving\n",
            "Giacomo_Medici,convicted of,conspiracy\n",
            "Giacomo_Medici,sentenced_to,eight years\n",
            "```\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Paragraph 9 from giacomo_namefixed_orgfixed_cleaned.txt:\n",
            "```csv\n",
            "subject,verb,object\n",
            "evidence,returned,artefacts\n",
            "museums,returned,artefacts\n",
            "collectors,returned,artefacts\n",
            "investigation,charged_with,Marion_True\n",
            "investigation,charged_with,Robert_Hecht\n",
            "```\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Extraction complete. Results saved in 'results' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## validate results\n",
        "\n",
        "This last bit runs your results through a checker to mark up rows that DO NOT have your target predicates present or DO NOT have 3 columns of data. This way, it becomes easy for you to manually inspect the results and decide how you want to handle things.\n",
        "\n",
        "If you change your list of desired predicates back where you extract things, make sure the 'valid_predicates' list below is updated accordingly.\n",
        "\n",
        "The code below will run on any file with _triplets.csv as part of the filename."
      ],
      "metadata": {
        "id": "qSrgGzFu8U0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "def error_check_predicates(input_file, output_file, valid_predicates=None):\n",
        "    \"\"\"\n",
        "    Check CSV file for valid predicates and column count.\n",
        "\n",
        "    Args:\n",
        "    input_file (str): Path to input CSV file\n",
        "    output_file (str): Path to output error-checked file\n",
        "    valid_predicates (list): List of valid predicate verbs\n",
        "    \"\"\"\n",
        "    with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
        "        reader = csv.reader(infile)\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        # Write header\n",
        "        header = next(reader)\n",
        "        writer.writerow(header)\n",
        "\n",
        "        for row in reader:\n",
        "            # Mark line with ### if more than 3 columns\n",
        "            if len(row) != 3:\n",
        "                row.insert(0, '###')\n",
        "                writer.writerow(row)\n",
        "                continue\n",
        "\n",
        "            # Check predicate validity - EXACT MATCH ONLY\n",
        "            verb = row[1].strip()\n",
        "            if verb not in valid_predicates:\n",
        "                row.insert(0, '###')\n",
        "\n",
        "            writer.writerow(row)\n",
        "\n",
        "def process_all_files(input_dir='results', output_dir='error-checked'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith('_triplets.csv'):\n",
        "            input_path = os.path.join(input_dir, filename)\n",
        "            output_path = os.path.join(output_dir, f'checked_{filename}')\n",
        "\n",
        "            error_check_predicates(input_path, output_path, valid_predicates=suggested_predicates) #### remember to change to whatever list of predicates you're using!\n",
        "            print(f'Processed:{output_dir}/{filename}')\n",
        "\n",
        "# Uncomment to process all files\n",
        "# process_all_files()\n",
        "\n",
        "# Direct call for testing\n",
        "#error_check_predicates('/content/results/giacomo_namefixed_cleaned_triplets.csv', 'giacomo_checked.txt', valid_predicates=predicates)\n",
        "#print(\"Now you can inspect the results; fix things, save, then change the file extension to .csv instead of .txt\")"
      ],
      "metadata": {
        "id": "czwfzoZJ3mhU"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_all_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmkaNGS5ii90",
        "outputId": "4b4ce62b-5826-4227-b585-172ff765d487"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed:error-checked/true_namefixed_orgfixed_cleaned_triplets.csv\n",
            "Processed:error-checked/giacomo_namefixed_orgfixed_cleaned_triplets.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#append .txt to files in the folder error-checked/\n",
        "#so we can check for errors\n",
        "\n",
        "import os\n",
        "\n",
        "def add_txt_extension(directory):\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".csv\"):\n",
        "            base_name, _ = os.path.splitext(filename)\n",
        "            new_filename = base_name + \".txt\"\n",
        "            old_path = os.path.join(directory, filename)\n",
        "            new_path = os.path.join(directory, new_filename)\n",
        "            os.rename(old_path, new_path)\n",
        "\n",
        "add_txt_extension(\"error-checked\")"
      ],
      "metadata": {
        "id": "EgdKw6-h5dmh"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can download your csv file with entities & relations, for use in knowledge graph embedding models, network analysis, or whatever else.\n",
        "\n",
        "Open the csv file in a text editor FIRST though, and sort the lines alphabetically. The lines marked with ### will be put at the top, and you can manually work through them to decide what to do with the predicate or the extra columns of data (where extra commas have crept in).\n",
        "\n",
        "Once you've saved your changes and everything is in well-formed csv, this last block below could be used to do some last data munging _on that fixed csv file_ in preparation for whatever you do next (eg, knowledge graph embedding model)."
      ],
      "metadata": {
        "id": "4lwow8E_55kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#append .txt to files in the folder error-checked/\n",
        "#so we can check for errors\n",
        "\n",
        "import os\n",
        "\n",
        "def add_csv_extension(directory):\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            base_name, _ = os.path.splitext(filename)\n",
        "            new_filename = base_name + \".csv\"\n",
        "            old_path = os.path.join(directory, filename)\n",
        "            new_path = os.path.join(directory, new_filename)\n",
        "            os.rename(old_path, new_path)\n",
        "\n",
        "add_csv_extension(\"error-checked\")"
      ],
      "metadata": {
        "id": "ZH31mPzi6fDr"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ok, so I've checked the 'giacomo' results manually (file was called giacomo_checked(2).csv), fixed the handful of relationships that got borked (which were mostly from not putting in a newline sometimes), now let's turn into gexf:"
      ],
      "metadata": {
        "id": "M-jKpd5rHhEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir finished"
      ],
      "metadata": {
        "id": "NAv11BOtkMt9"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def process_csv_files(input_folder, output_folder=None):\n",
        "    # Create output folder if not specified\n",
        "    if output_folder is None:\n",
        "        output_folder = input_folder\n",
        "\n",
        "    # Ensure output folder exists\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Iterate through all files in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        # Process only CSV files\n",
        "        if filename.endswith('.csv'):\n",
        "            # Full input file path\n",
        "            input_path = os.path.join(input_folder, filename)\n",
        "\n",
        "            # Read the CSV file\n",
        "            thing = pd.read_csv(input_path, names=['subject', 'verb', 'object'], header=0, on_bad_lines='skip')\n",
        "\n",
        "\n",
        "            # Drop rows with NaN values\n",
        "            thing = thing.dropna()\n",
        "\n",
        "            # Replace spaces with underscores in the specified columns\n",
        "            thing['subject'] = thing['subject'].str.replace(' ', '_')\n",
        "            thing['verb'] = thing['verb'].str.replace(' ', '_')\n",
        "            thing['object'] = thing['object'].str.replace(' ', '_')\n",
        "\n",
        "            # Lowercase only specific columns\n",
        "            # This will preserve the case of the 'verb' column\n",
        "            for col in [thing.columns[0], thing.columns[-1]]:  # subject and object columns\n",
        "                thing[col] = thing[col].astype(str).str.lower()\n",
        "\n",
        "            # Remove quotation marks from the first and last columns\n",
        "            for col in [thing.columns[0], thing.columns[-1]]:\n",
        "                thing.loc[:, col] = thing[col].astype(str).str.strip('\"')\n",
        "\n",
        "            # Create output filename (optionally add a prefix or suffix)\n",
        "            output_filename = f\"processed_{filename}\"\n",
        "            output_path = os.path.join(output_folder, output_filename)\n",
        "\n",
        "            # Save the processed DataFrame to a new CSV file\n",
        "            thing.to_csv(output_path, index=False)\n",
        "\n",
        "            print(f\"Processed: {filename} → {output_filename}\")\n",
        "\n",
        "# Usage example\n",
        "process_csv_files('error-checked', 'finished')"
      ],
      "metadata": {
        "id": "J8_ibQuVeuCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a9d925-2352-4d09-9509-bdce8f58382b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed: checked_true_namefixed_orgfixed_cleaned_triplets.csv → processed_checked_true_namefixed_orgfixed_cleaned_triplets.csv\n",
            "Processed: checked_giacomo_namefixed_orgfixed_cleaned_triplets.csv → processed_checked_giacomo_namefixed_orgfixed_cleaned_triplets.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Experimental!\n",
        "### template for inferring - for the machine - relations obvious to the reader\n",
        "!llm --system \"Analyze the existing relationship graph in the input CSV. Your task is to DEDUCE missing relationships ONLY for entities already present in the graph, using logical inference and contextual understanding.  Constraints: 1. ONLY propose relationships between entities ALREADY mentioned in the existing data 2. Base inferences on historical context, professional relationships, and known interactions 3. Provide high-confidence deductions that are strongly supported by implicit connections  Reasoning Guidelines: - Look for implied but not explicitly stated connections - Consider professional networks, geographical associations, and documented interactions - Avoid speculative or far-fetched relationships  Output Format: subject,verb,object  Example Reasoning: - If an antiquities dealer is known to work with a specific gallery, infer ownership or operational connection - If multiple people appear in legal proceedings, infer potential collaborative or antagonistic relationships - If an artifact is traced to a specific location, infer provenance or ownership connections  RETURN ONLY THE DEDUCED TRIPLES IN CSV FORMAT. No explanatory text. Provide a comment with # indicating WHY\" --save infer"
      ],
      "metadata": {
        "id": "srhvsXbWmR4s"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### EXperimental!\n",
        "\n",
        "#! cat giacomo_finished_triplets.csv | llm -m llama3 'Given these relationships, DEDUCE relationships that are missing ONLY FOR ENTITIES ALREADY IN THE GRAPH. For instance, if someone is a known actor, and there is an entity called {storeroom of person}, we could assume a connection. Return likely triples.'\n",
        "\n",
        "!cat /content/error-checked/checked_giacomo_namefixed_orgfixed_cleaned_triplets.csv | llm -m themodel -t infer\n",
        "!cat /content/error-checked/checked_true_namefixed_orgfixed_cleaned_triplets.csv | llm -m themodel -c -t infer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfQqFOGRJlHE",
        "outputId": "406707e4-0f88-4e3a-a299-52af92dbaf9f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tombaroli,supplied,Giacomo_Medici #Tomb robbers excavated the krater that Medici sold, implying a supply chain.\n",
            "Robert_Hecht,associated_with,Giacomo_Medici #Hecht bought the krater from Medici, implying a business relationship.\n",
            "Christian_Boursaud,associated_with,Giacomo_Medici #Boursaud and Medici both consigned material and Medici supplied material, suggesting a business connection.\n",
            "Sotheby’s_London,sold,material #Sotheby's received material that Medici supplied and they sold vases, suggesting the material included vases.\n",
            "Editions_Services,published,material #Medici bought Editions Services and consigned material, implying a publishing relationship.\n",
            "Sotheby’s_London,associated_with,Giacomo_Medici #Sotheby's handled material from Medici and Editions Services, implying a business relationship.\n",
            "Robin_Symes,associated_with,Robert_Hecht #Symes, Hecht, and others were associated with Medici, who dealt in illicit antiquities, suggesting a shared network.\n",
            "Frieda_Tchacos,associated_with,Robert_Hecht #Tchacos, Hecht, and others were associated with Medici, who dealt in illicit antiquities, suggesting a shared network.\n",
            "Nikolas_Koutoulakis,associated_with,Robert_Hecht #Koutoulakis, Hecht, and others were associated with Medici, who dealt in illicit antiquities, suggesting a shared network.\n",
            "Ali_Aboutaam,associated_with,Robert_Hecht #Aboutaam, Hecht, and others were associated with Medici, who dealt in illicit antiquities, suggesting a shared network.\n",
            "Hicham_Aboutaam,associated_with,Robert_Hecht #Aboutaam, Hecht, and others were associated with Medici, who dealt in illicit antiquities, suggesting a shared network.\n",
            "Giacomo_Medici,operated,Rome gallery #Medici closed his Rome gallery, implying prior operation.\n",
            "Giacomo_Medici,stored,material,Geneva_Freeport #Medici's storage spaces and storerooms were in Geneva Freeport where he stored his material.\n",
            "Giacomo_Medici,supplied,objects #Medici supplied material and is connected to objects consigned by Boursaud, suggesting the material might be the objects.\n",
            "krater,located_at,Italy #The krater was looted, and much of Medici's activity was in Italy.\n",
            "tombaroli,located_at,Italy #The Euphronios Krater was found in an Etruscan tomb in Italy and tombaroli are tomb robbers.\n",
            "Sotheby’s_London,associated_with,Editions_Services #Medici bought Editions_Services, consigned material, Sotheby’s received material and sold vases, implying a connection.\n",
            "Giacomo_Medici,supplied,sarcophagus fragment #Medici trafficked sarcophagus fragment, implies he supplied it to others.\n",
            "Giacomo_Medici,supplied,sarcophagus #Medici trafficked in looted artifacts and the sarcophagus was stolen then sold by Sotheby's who Medici worked with.\n",
            "\n",
            "Jiri_Frel,worked_at,J_Paul_Getty_Museum #Frel supervised True, indicating employment at the Getty.\n",
            "J_Paul_Getty_Museum,returned,Kanakaria mosaics #True rejected the mosaics, and the Getty later returned several pieces, implying they were acquired.\n",
            "J_Paul_Getty_Museum,returned,Sevso Treasure #True rejected the treasure, and the Getty later returned several pieces, implying they were acquired.\n",
            "Marion_True,influenced,policy guidelines #True's rejections likely led to the Getty adopting stricter guidelines.\n",
            "Marion_True,worked_at,Getty Villa #The Getty Villa's shift was envisaged, and True suggested revisions, indicating her involvement.\n",
            "Christos_Michaelides,loaned_to,Marion_True #True borrowed from and repaid Michaelides.\n",
            "Barbara_and_Lawrence_Fleischmans,loaned_to,Marion_True #True borrowed from the Fleischmans.\n",
            "Robin_Symes,supplied,J_Paul_Getty_Museum #The Getty bought from Symes, who was a known supplier of antiquities.\n",
            "Marion_True,oversaw,acquisitions #True's role as curator involved acquisitions, which were approved by various figures.\n",
            "J_Paul_Getty_Museum,acquired,objects #Medici handled objects, True acquired objects, and the Getty acquired objects, suggesting a connection.\n",
            "Giacomo_Medici,supplied,J_Paul_Getty_Museum #Medici handled objects that the Getty acquired, suggesting a supply chain.\n",
            "Marion_True,associated_with,Robin_Symes #True and Symes were both involved with the Getty's acquisition of antiquities.\n",
            "J_Paul_Getty_Museum,displayed,Barbara_and_Lawrence_Fleischman_Collection #The Getty acquired and published the Fleischman collection, implying they displayed it.\n",
            "Marion_True,influenced,acquisitions #True encouraged the Fleischmans and acquired objects, suggesting influence on Getty's acquisitions.\n",
            "Harold_Williams,worked_at,J_Paul_Getty_Museum #Williams approved acquisitions, indicating a position at the Getty.\n",
            "Barry_Munitz,worked_at,J_Paul_Getty_Museum #Munitz approved acquisitions, indicating a position at the Getty.\n",
            "John_Walsh,worked_at,J_Paul_Getty_Museum #Walsh approved acquisitions, indicating a position at the Getty.\n",
            "Deborah_Gribbon,worked_at,J_Paul_Getty_Museum #Gribbon approved acquisitions, indicating a position at the Getty.\n",
            "Board_of_Trustees,governed,J_Paul_Getty_Museum #The Board of Trustees approved Getty acquisitions, implying governance.\n",
            "Giacomo_Medici,supplied,Marion_True  #Medici's seized materials connected him to True and her acquisitions.\n",
            "bronze tripod and candelabrum,part_of,Guglielmo_Collection #The tripod and candelabrum were stolen from the Guglielmo Collection.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def concatenate_csv_files(input_folder, output_filename='combined_output.csv'):\n",
        "    # List to store all dataframes\n",
        "    all_dataframes = []\n",
        "\n",
        "    # Flag to track whether headers have been processed\n",
        "    first_file = True\n",
        "\n",
        "    # Iterate through all files in the input folder\n",
        "    for filename in sorted(os.listdir(input_folder)):\n",
        "        # Process only CSV files\n",
        "        if filename.endswith('.csv'):\n",
        "            # Full input file path\n",
        "            input_path = os.path.join(input_folder, filename)\n",
        "\n",
        "            # Read the CSV file\n",
        "            if first_file:\n",
        "                # For the first file, read with headers\n",
        "                df = pd.read_csv(input_path)\n",
        "                all_dataframes.append(df)\n",
        "                first_file = False\n",
        "            else:\n",
        "                # For subsequent files, skip the header row\n",
        "                df = pd.read_csv(input_path, header=None, skiprows=1)\n",
        "\n",
        "                # Ensure the columns match the first file\n",
        "                if len(df.columns) == len(all_dataframes[0].columns):\n",
        "                    df.columns = all_dataframes[0].columns\n",
        "                    all_dataframes.append(df)\n",
        "                else:\n",
        "                    print(f\"Skipping {filename}: Column mismatch\")\n",
        "\n",
        "    # Concatenate all dataframes\n",
        "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "    # Save the combined dataframe\n",
        "    combined_df.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"Combined {len(all_dataframes)} files into {output_filename}\")\n",
        "    print(f\"Total rows: {len(combined_df)}\")\n",
        "\n",
        "# Usage example\n",
        "concatenate_csv_files('finished', 'final_combined_output.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bovVw59k4oi",
        "outputId": "82f903f5-e372-48ab-c7db-4c293bba8561"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined 2 files into final_combined_output.csv\n",
            "Total rows: 116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, something like Open Refine would be a good idea, so that 'Getty', 'Getty Museum', 'J. Paul Getty' etc all get smooshed into a single node. Then progress to visualize or kg-embedding and so on."
      ],
      "metadata": {
        "id": "WRzDr1PSlljf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize via GEXF"
      ],
      "metadata": {
        "id": "ipbQv6MreSQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import xml.etree.ElementTree as ET\n",
        "import xml.dom.minidom as minidom\n",
        "\n",
        "def csv_to_gexf(input_csv, output_gexf, source_col='source', target_col='target', weight_col=None, relationship_col='relationship'):\n",
        "    \"\"\"\n",
        "    Convert a CSV file to a GEXF network file.\n",
        "\n",
        "    Args:\n",
        "        input_csv (str): Path to the input CSV file.\n",
        "        output_gexf (str): Path to the output GEXF file.\n",
        "        source_col (str, optional): Name of the source node column. Defaults to 'source'.\n",
        "        target_col (str, optional): Name of the target node column. Defaults to 'target'.\n",
        "        weight_col (str, optional): Name of the weight column. Defaults to None.\n",
        "        relationship_col (str, optional): Name of the relationship column (used for edge labels). Defaults to 'relationship'.\n",
        "\n",
        "    Returns:\n",
        "        networkx.Graph: The created network graph.\n",
        "    \"\"\"\n",
        "    # Read the CSV file\n",
        "    try:\n",
        "        df = pd.read_csv(input_csv)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Validate required columns\n",
        "    if source_col not in df.columns or target_col not in df.columns:\n",
        "        print(f\"Error: Required columns {source_col} or {target_col} not found in the CSV.\")\n",
        "        return None\n",
        "\n",
        "    # Create a graph\n",
        "    G = nx.from_pandas_edgelist(\n",
        "        df,\n",
        "        source=source_col,\n",
        "        target=target_col,\n",
        "        edge_attr=([weight_col] if weight_col and weight_col in df.columns else None)\n",
        "    )\n",
        "\n",
        "    # Prepare GEXF XML structure\n",
        "    gexf = ET.Element('gexf', {\n",
        "        'xmlns': 'http://www.gexf.net/1.2draft',\n",
        "        'xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance',\n",
        "        'xsi:schemaLocation': 'http://www.gexf.net/1.2draft http://www.gexf.net/1.2draft/gexf.xsd',\n",
        "        'version': '1.2'\n",
        "    })\n",
        "\n",
        "    # Meta information\n",
        "    meta = ET.SubElement(gexf, 'meta')\n",
        "    ET.SubElement(meta, 'creator').text = 'CSV to GEXF Converter'\n",
        "    ET.SubElement(meta, 'description').text = f'Network generated from {input_csv}'\n",
        "\n",
        "    # Graph element\n",
        "    graph = ET.SubElement(gexf, 'graph', {'defaultedgetype': 'undirected'})\n",
        "\n",
        "    # Nodes\n",
        "    nodes = ET.SubElement(graph, 'nodes')\n",
        "    for i, node in enumerate(G.nodes()):\n",
        "        node_elem = ET.SubElement(nodes, 'node', {\n",
        "            'id': str(node),\n",
        "            'label': str(node)\n",
        "        })\n",
        "\n",
        "    # Edges\n",
        "    edges = ET.SubElement(graph, 'edges')\n",
        "    for i, (source, target, data) in enumerate(G.edges(data=True)):\n",
        "        # Find the corresponding row in the original DataFrame\n",
        "        matching_row = df[(df[source_col] == source) & (df[target_col] == target)]\n",
        "\n",
        "        edge_attrs = {\n",
        "            'id': str(i),\n",
        "            'source': str(source),\n",
        "            'target': str(target)\n",
        "        }\n",
        "\n",
        "        # Add weight if available\n",
        "        if weight_col and weight_col in data:\n",
        "            edge_attrs['weight'] = str(data[weight_col])\n",
        "\n",
        "        # Add relationship label if column exists\n",
        "        if relationship_col in df.columns and not matching_row.empty:\n",
        "            relationship_value = matching_row[relationship_col].iloc[0]\n",
        "            edge_attrs['label'] = str(relationship_value)\n",
        "\n",
        "        ET.SubElement(edges, 'edge', edge_attrs)\n",
        "\n",
        "    # Convert to pretty-printed XML\n",
        "    rough_string = ET.tostring(gexf, 'utf-8')\n",
        "    reparsed = minidom.parseString(rough_string)\n",
        "\n",
        "    # Write to file\n",
        "    with open(output_gexf, 'w', encoding='utf-8') as f:\n",
        "        f.write(reparsed.toprettyxml(indent=\"  \"))\n",
        "\n",
        "    print(f\"GEXF file created successfully: {output_gexf}\")\n",
        "    print(f\"Network stats - Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
        "\n",
        "    return G"
      ],
      "metadata": {
        "id": "iRR_S3EgHZYp"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_to_gexf(\n",
        "    input_csv='final_combined_output.csv',\n",
        "    output_gexf='final_combined_output.gexf',\n",
        "    source_col='subject',\n",
        "    target_col='object',\n",
        "    relationship_col='verb'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0awM5GTIOX1",
        "outputId": "0febf739-7385-4656-b7a0-931aa8496e9e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEXF file created successfully: final_combined_output.gexf\n",
            "Network stats - Nodes: 139, Edges: 145\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<networkx.classes.graph.Graph at 0x79eba72eada0>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## send it to neo4j?\n",
        "\n",
        "I've got some code somewhere that ought to generate neo4j, maybe that'd be useful"
      ],
      "metadata": {
        "id": "FPsRVG0jeWto"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xjyhc2Y3eWWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "B74OdSYjMXLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now Let's Try Nuextract!\n",
        "\n",
        "Nuextract is a small model fine-tuned on extracting structured data. But a lot depends on finding the exact right template structure to do it. But let's see what we can do. I suspect having done the pre-processing on the input text will help things.\n",
        "\n",
        "Requires a Huggingface token, saved in secrets here as HF_TOKEN"
      ],
      "metadata": {
        "id": "yuUUQfOQL4eA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "see also this notebook https://colab.research.google.com/drive/15SL9vCumXvAkoqn2va5b5vhmRVYu9arO?usp=sharing#scrollTo=e8SU1yEb0yI6"
      ],
      "metadata": {
        "id": "OPCs3h_fOHow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### code for calling the model\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def predict_NuExtract(model, tokenizer, texts, template, batch_size=1, max_length=10_000, max_new_tokens=4_000):\n",
        "    template = json.dumps(json.loads(template), indent=4)\n",
        "    prompts = [f\"\"\"<|input|>\\n### Template:\\n{template}\\n### Text:\\n{text}\\n\\n<|output|>\"\"\" for text in texts]\n",
        "\n",
        "    outputs = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(prompts), batch_size):\n",
        "            batch_prompts = prompts[i:i+batch_size]\n",
        "            batch_encodings = tokenizer(batch_prompts, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length).to(model.device)\n",
        "\n",
        "            pred_ids = model.generate(**batch_encodings, max_new_tokens=max_new_tokens)\n",
        "            outputs += tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "    return [output.split(\"<|output|>\")[1] for output in outputs]\n",
        "\n",
        "# function for writing json to csv\n",
        "\n",
        "import csv\n",
        "\n",
        "def json_to_csv(json_data, output_file='nuextract_output.csv'):\n",
        "    # Parse JSON if it's a string\n",
        "    if isinstance(json_data, str):\n",
        "        json_data = json.loads(json_data)\n",
        "\n",
        "    # Flatten nested JSON\n",
        "    def flatten_json(data, prefix=''):\n",
        "        flat_dict = {}\n",
        "        for key, value in data.items():\n",
        "            new_key = f\"{prefix}{key}\" if prefix else key\n",
        "\n",
        "            if isinstance(value, dict):\n",
        "                flat_dict.update(flatten_json(value, f\"{new_key}_\"))\n",
        "            elif isinstance(value, list):\n",
        "                flat_dict[new_key] = ', '.join(map(str, value))\n",
        "            else:\n",
        "                flat_dict[new_key] = value\n",
        "        return flat_dict\n",
        "\n",
        "    # Flatten the JSON\n",
        "    flat_data = flatten_json(json_data)\n",
        "\n",
        "    # Write to CSV\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=flat_data.keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerow(flat_data)\n",
        "\n",
        "    print(f\"CSV file '{output_file}' has been created.\")"
      ],
      "metadata": {
        "id": "DtIqMR57MhRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the smallest model which fits in colab\n",
        "# the only nuextract model that works in vanilla colab is the tiny one. try using smol via Ollama w/ temperature at 0.\n",
        "#model_name = \"numind/NuExtract-tiny-v1.5\"\n",
        "model_name = \"numind//NuExtract-1.5\"\n",
        "device = \"cuda\"\n",
        "\n",
        "## model is used with llm, so maybe I need a new name here?\n",
        "hfmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, trust_remote_code=True).to(device).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n"
      ],
      "metadata": {
        "id": "_6jveHbMMwfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "looting_template = \"\"\"{\n",
        "    \"Antiquities_Trade_Record\": {\n",
        "        \"Entities\": {\n",
        "            \"Actors\": [\n",
        "                {\n",
        "                    \"name\": \"\",\n",
        "                    \"type\": \"\",\n",
        "                    \"nationality\": \"\",\n",
        "                    \"roles\": []\n",
        "                }\n",
        "            ],\n",
        "            \"Organizations\": [\n",
        "                {\n",
        "                    \"name\": \"\",\n",
        "                    \"type\": \"\",\n",
        "                    \"location\": \"\",\n",
        "                    \"established_year\": \"\"\n",
        "                }\n",
        "            ],\n",
        "            \"Institutions\": [\n",
        "                {\n",
        "                    \"name\": \"\",\n",
        "                    \"type\": \"\",\n",
        "                    \"jurisdiction\": \"\",\n",
        "                    \"founding_date\": \"\"\n",
        "                }\n",
        "            ],\n",
        "            \"Objects\": [\n",
        "                {\n",
        "                    \"name\": \"\",\n",
        "                    \"type\": \"\",\n",
        "                    \"origin\": \"\",\n",
        "                    \"estimated_value\": \"\",\n",
        "                    \"date_of_creation\": \"\",\n",
        "                    \"cultural_origin\": \"\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        \"Relationships\": {\n",
        "            \"involvedIn\": [\n",
        "                {\n",
        "                    \"entity1\": \"\",\n",
        "                    \"entity2\": \"\",\n",
        "                    \"role\": \"\",\n",
        "                    \"date\": \"\",\n",
        "                    \"details\": \"\"\n",
        "                }\n",
        "            ],\n",
        "            \"transactedWith\": [\n",
        "                {\n",
        "                    \"buyer\": \"\",\n",
        "                    \"seller\": \"\",\n",
        "                    \"object\": \"\",\n",
        "                    \"transaction_date\": \"\",\n",
        "                    \"transaction_value\": \"\",\n",
        "                    \"location\": \"\"\n",
        "                }\n",
        "            ],\n",
        "            \"connectedTo\": [\n",
        "                {\n",
        "                    \"entity1\": \"\",\n",
        "                    \"entity2\": \"\",\n",
        "                    \"connection_type\": \"\",\n",
        "                    \"strength_of_connection\": \"\",\n",
        "                    \"time_period\": \"\"\n",
        "                }\n",
        "            ],\n",
        "            \"legalStatus\": [\n",
        "                {\n",
        "                    \"entity\": \"\",\n",
        "                    \"status\": \"\",\n",
        "                    \"jurisdiction\": \"\",\n",
        "                    \"date_of_status_change\": \"\",\n",
        "                    \"legal_details\": \"\"\n",
        "                }\n",
        "            ],\n",
        "            \"originatedFrom\": [\n",
        "                {\n",
        "                    \"object\": \"\",\n",
        "                    \"original_location\": \"\",\n",
        "                    \"excavation_date\": \"\",\n",
        "                    \"archaeological_context\": \"\"\n",
        "                }\n",
        "            ],\n",
        "            \"operatedIn\": [\n",
        "                {\n",
        "                    \"entity\": \"\",\n",
        "                    \"geographic_region\": \"\",\n",
        "                    \"time_period\": \"\",\n",
        "                    \"operational_details\": \"\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        \"Context\": {\n",
        "            \"source_document\": \"\",\n",
        "            \"date_of_record\": \"\",\n",
        "            \"additional_notes\": \"\"\n",
        "        }\n",
        "    }\n",
        "}\"\"\""
      ],
      "metadata": {
        "id": "hexX5DcCN3Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(\"giacomo_namefixed.txt\", \"r\").read()"
      ],
      "metadata": {
        "id": "B5beS6WEMH7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict!\n",
        "prediction = predict_NuExtract(hfmodel, tokenizer, [text], looting_template)[0]\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "eYpbYHFVN_1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_to_csv(prediction)"
      ],
      "metadata": {
        "id": "xXiWBXbHOdoI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}